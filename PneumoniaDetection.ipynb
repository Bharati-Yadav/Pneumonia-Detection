{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PneumoniaDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "srFEJkPcuyBR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e0069e6-89ad-44d4-cf37-37b9c24fc49c"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.applications.resnet_v2 import ResNet50V2\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from glob import glob\n",
        "from keras.layers import Input, Lambda, Dense, Flatten"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLgIGhLNvIbL",
        "colab_type": "code",
        "outputId": "16029489-5808-41f6-b2ec-b1714681c52d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRjZPuv-u0vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/My Drive/chest_xray.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCwBx_CWvOFZ",
        "colab_type": "code",
        "outputId": "cd5b829e-61c5-4e9f-e348-865cc8b5990d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "import os\n",
        "os.listdir()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'chest_xray',\n",
              " 'drive',\n",
              " 'chest_fp32_model.pb',\n",
              " 'Convert_tensor_rt.h5',\n",
              " 'chest_fp6_model.pb',\n",
              " 'model',\n",
              " '__MACOSX',\n",
              " 'chest_TensorRT_int8.pb',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJl5BtONvcgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_SIZE=[224, 224]\n",
        "train_data_dir = 'chest_xray/train'\n",
        "validation_data_dir = 'chest_xray/test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKknQn7nvlOK",
        "colab_type": "code",
        "outputId": "e243d2aa-1b7d-4b8e-b47a-e852bbd52fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "res = ResNet50V2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZiq6BYnJ8VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in res.layers:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puGnLYD2KQcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = glob('chest_xray/train/*')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6f-MMpwKU6d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Flatten()(res.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folder), activation='softmax')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHSB6c6UKYQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=res.input, outputs=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E37sCHhHKcnC",
        "colab_type": "code",
        "outputId": "ab56198a-f3ea-469e-f4e5-ba15c82721e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkm1oMAxKhiv",
        "colab_type": "code",
        "outputId": "dcd428b3-9090-4e42-e998-19778e00dfd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "epochs = 2\n",
        "batch_size = 40\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('chest_xray/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('chest_xray/test',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'categorical')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxtJJu1nKmAX",
        "colab_type": "code",
        "outputId": "61dec30e-74ee-4e17-f2a7-897a7c641435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "%%time\n",
        "model.fit_generator(\n",
        "    training_set,\n",
        "    #steps_per_epoch = train_generator.n//batch_size,\n",
        "    epochs = epochs,\n",
        "    steps_per_epoch=20,\n",
        "    validation_data=test_set\n",
        "\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/2\n",
            "20/20 [==============================] - 27s 1s/step - loss: 0.7734 - acc: 0.8900 - val_loss: 3.9542 - val_acc: 0.6987\n",
            "Epoch 2/2\n",
            "20/20 [==============================] - 21s 1s/step - loss: 0.5188 - acc: 0.9488 - val_loss: 5.4581 - val_acc: 0.6378\n",
            "CPU times: user 1min 9s, sys: 1.34 s, total: 1min 11s\n",
            "Wall time: 47.9 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbf79b420f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHR7w13cKrCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/Convert_tensor_rt.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4PlK7z2OkTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "# This line must be executed before loading Keras model.\n",
        "K.set_learning_phase(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8pW16K-Ori5",
        "colab_type": "code",
        "outputId": "90585b52-e5e5-4744-d972-14f7da3907f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/Convert_tensor_rt.h5')\n",
        "print(model.outputs)\n",
        "# [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]\n",
        "print(model.inputs)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "[<tf.Tensor 'dense_1/Softmax:0' shape=(?, 2) dtype=float32>]\n",
            "[<tf.Tensor 'input_1:0' shape=(?, 224, 224, 3) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXUZU1OtOxch",
        "colab_type": "code",
        "outputId": "1b124b79-19e2-4297-fbcf-ec1d213046cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        # Graph -> GraphDef ProtoBuf\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
        "                                                      output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "\n",
        "\n",
        "frozen_graph = freeze_session(K.get_session(),\n",
        "                              output_names=[out.op.name for out in model.outputs])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-597375817328>:32: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 281 variables.\n",
            "INFO:tensorflow:Converted 281 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4BCZ8Z9O6dX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6a4b32f-fec1-47e1-cdb6-cdb9b56912d0"
      },
      "source": [
        "tf.train.write_graph(frozen_graph, \"model\", \"tf_model.pb\", as_text=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model/tf_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1aFyjRJuvaa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b53a53f0-6774-457d-aff3-f42ac0b9d9fc"
      },
      "source": [
        "#converting model to tensorrt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import gfile\n",
        "sess = tf.Session()\n",
        "f = gfile.FastGFile(\"./model/tf_model.pb\", 'rb')\n",
        "graph_def = tf.GraphDef()\n",
        "# Parses a serialized binary message into the current message.\n",
        "graph_def.ParseFromString(f.read())\n",
        "f.close()\n",
        "\n",
        "sess.graph.as_default()\n",
        "# Import a serialized TensorFlow `GraphDef` protocol buffer\n",
        "# and place into the current default `Graph`.\n",
        "tf.import_graph_def(graph_def)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-a1ae84b6b2c5>:4: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.gfile.GFile.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVdJyLB4u282",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "84bdca29-6d9b-49ff-8be4-9a3f4fa8ac71"
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "converter = trt.TrtGraphConverter(\n",
        "\tinput_graph_def=graph_def,\n",
        "\tnodes_blacklist=['dense_1/Softmax:0'])\n",
        "frozen_graph = converter.convert()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8JxdWZLvCpC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52fea186-ddd4-435b-9ad1-7873ba72b210"
      },
      "source": [
        "tf.train.write_graph(graph_def, \"model\", \"tf_trt_model.pb\", as_text=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model/tf_trt_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fUhlSanvHR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c4e955a7-428c-439e-a138-3e8b261de9b5"
      },
      "source": [
        "all_nodes=len([1 for n in frozen_graph.node])\n",
        "print(\"no. of all_nodes in frozen graph: \",all_nodes)\n",
        "trt_engine_nodes = len([1 for n in graph_def.node if str(n.op)=='TRTeng'])\n",
        "print(\"no. of trt_engine_nodes in tensorrt graph: \",trt_engine_nodes)\n",
        "all_nodes=len([1 for n in graph_def.node])\n",
        "print(\"no. of all_nodes in tensorrt graph: \",all_nodes)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of all_nodes in frozen graph:  1548\n",
            "no. of trt_engine_nodes in tensorrt graph:  0\n",
            "no. of all_nodes in tensorrt graph:  1628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmxeWhuQvYVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V69WqRikvchT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "files = os.listdir('chest_xray/test/PNEUMONIA/')\n",
        "img1 = image.load_img(r'chest_xray/test/PNEUMONIA/' + files[0], target_size=(224, 224))\n",
        "img_array1 = image.img_to_array(img1)\n",
        "img_array_expanded_dims1 = np.expand_dims(img_array1, axis=0)\n",
        "input_img = preprocess_input(img_array_expanded_dims1)\n",
        "\n",
        "for i in files[1:70]:\n",
        "    img2 = image.load_img('chest_xray/test/PNEUMONIA/' + i, target_size=(224, 224))\n",
        "    img_array2 = image.img_to_array(img2)\n",
        "    img_array_expanded_dims2 = np.expand_dims(img_array2, axis=0)\n",
        "    img2 = preprocess_input(img_array_expanded_dims2)\n",
        "\n",
        "    input_img = np.concatenate((input_img, img2),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ8Y9zyQvhTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to read a \".pb\" model \n",
        "# (can be used to read frozen model or TensorRT model)\n",
        "def read_pb_graph(model):\n",
        "  with gfile.FastGFile(model,'rb') as f:\n",
        "    graph_def = tf.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "  return graph_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lxfGMUIvm3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "34b7ca03-9b21-45a6-bd48-d77a43ef45fc"
      },
      "source": [
        "TENSORRT_MODEL_PATH = \"/content/model/tf_trt_model.pb\"\n",
        "import time\n",
        "import tensorflow as tf\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "        # read TensorRT model\n",
        "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
        "\n",
        "        # obtain the corresponding input-output tensor\n",
        "        tf.import_graph_def(trt_graph, name='')\n",
        "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
        "        output = sess.graph.get_tensor_by_name('dense_1/Softmax:0')\n",
        "\n",
        "        # in this case, it demonstrates to perform inference for 50 times\n",
        "        total_time = 0; n_time_inference = 50\n",
        "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
        "        for i in range(n_time_inference):\n",
        "            t1 = time.time()\n",
        "            out_pred_ = sess.run(output, feed_dict={input: input_img})\n",
        "            t2 = time.time()\n",
        "            delta_time = t2 - t1\n",
        "            total_time += delta_time\n",
        "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
        "        avg_time_tensorRT = total_time / n_time_inference\n",
        "        print(\"average inference time : \", avg_time_tensorRT)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needed time in inference-0:  0.10517311096191406\n",
            "needed time in inference-1:  0.10358643531799316\n",
            "needed time in inference-2:  0.10381317138671875\n",
            "needed time in inference-3:  0.10467100143432617\n",
            "needed time in inference-4:  0.10370135307312012\n",
            "needed time in inference-5:  0.10396289825439453\n",
            "needed time in inference-6:  0.10717201232910156\n",
            "needed time in inference-7:  0.10359740257263184\n",
            "needed time in inference-8:  0.10460042953491211\n",
            "needed time in inference-9:  0.1043543815612793\n",
            "needed time in inference-10:  0.10494828224182129\n",
            "needed time in inference-11:  0.10386157035827637\n",
            "needed time in inference-12:  0.10411858558654785\n",
            "needed time in inference-13:  0.10403323173522949\n",
            "needed time in inference-14:  0.1040339469909668\n",
            "needed time in inference-15:  0.10443305969238281\n",
            "needed time in inference-16:  0.10484194755554199\n",
            "needed time in inference-17:  0.10443592071533203\n",
            "needed time in inference-18:  0.10449934005737305\n",
            "needed time in inference-19:  0.10557270050048828\n",
            "needed time in inference-20:  0.10872697830200195\n",
            "needed time in inference-21:  0.10408782958984375\n",
            "needed time in inference-22:  0.10404515266418457\n",
            "needed time in inference-23:  0.10428357124328613\n",
            "needed time in inference-24:  0.10419750213623047\n",
            "needed time in inference-25:  0.10543107986450195\n",
            "needed time in inference-26:  0.10480570793151855\n",
            "needed time in inference-27:  0.10364198684692383\n",
            "needed time in inference-28:  0.1035006046295166\n",
            "needed time in inference-29:  0.1046748161315918\n",
            "needed time in inference-30:  0.10384106636047363\n",
            "needed time in inference-31:  0.10389471054077148\n",
            "needed time in inference-32:  0.10446739196777344\n",
            "needed time in inference-33:  0.10481882095336914\n",
            "needed time in inference-34:  0.10630321502685547\n",
            "needed time in inference-35:  0.10398054122924805\n",
            "needed time in inference-36:  0.10374879837036133\n",
            "needed time in inference-37:  0.1037442684173584\n",
            "needed time in inference-38:  0.10442543029785156\n",
            "needed time in inference-39:  0.10352182388305664\n",
            "needed time in inference-40:  0.10357856750488281\n",
            "needed time in inference-41:  0.1042790412902832\n",
            "needed time in inference-42:  0.10393095016479492\n",
            "needed time in inference-43:  0.10488367080688477\n",
            "needed time in inference-44:  0.10756254196166992\n",
            "needed time in inference-45:  0.104888916015625\n",
            "needed time in inference-46:  0.10546875\n",
            "needed time in inference-47:  0.10385942459106445\n",
            "needed time in inference-48:  0.10441851615905762\n",
            "needed time in inference-49:  0.10314035415649414\n",
            "average inference time :  0.10451125621795654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B22odtZPv5tZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "dbe5534f-7a47-42e5-8195-04f28f0d55a3"
      },
      "source": [
        "# convert (optimize) frozen model to TensorRT model......FP32\n",
        "trt_graph = trt.create_inference_graph(\n",
        "    input_graph_def=frozen_graph,        # frozen model\n",
        "    outputs=['dense_1/Softmax:0'],\n",
        "    max_batch_size=70,           # specify your max batch size\n",
        "    max_workspace_size_bytes=2*(10**9),    # specify the max workspace\n",
        "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
        "\n",
        "#write the TensorRT model to be used later for inference\n",
        "with gfile.FastGFile(\"chest_fp32_model.pb\", 'wb') as f:\n",
        "    f.write(trt_graph.SerializeToString())\n",
        "print(\"TensorRT model is successfully stored!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
            "TensorRT model is successfully stored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8fs0JA2wNUw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "5074067b-e75b-4447-b251-3b96d90b83e3"
      },
      "source": [
        "import time\n",
        "# variable\n",
        "TENSORRT_MODEL_PATH = 'chest_fp32_model.pb'\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "        # read TensorRT model\n",
        "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
        "\n",
        "        # obtain the corresponding input-output tensor\n",
        "        tf.import_graph_def(trt_graph, name='')\n",
        "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
        "        output = sess.graph.get_tensor_by_name('dense_1/Softmax:0')\n",
        "\n",
        "        # in this case, it demonstrates to perform inference for 50 times\n",
        "        total_time = 0; n_time_inference = 50\n",
        "        out_pred32 = sess.run(output, feed_dict={input: input_img})\n",
        "        for i in range(n_time_inference):\n",
        "            t1 = time.time()\n",
        "            out_pred_32 = sess.run(output, feed_dict={input: input_img})\n",
        "            t2 = time.time()\n",
        "            delta_time = t2 - t1\n",
        "            total_time += delta_time\n",
        "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
        "        avg_time_tensorRT_32 = total_time / n_time_inference\n",
        "        print(\"average inference time of fp32: \", avg_time_tensorRT_32)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needed time in inference-0:  0.10667896270751953\n",
            "needed time in inference-1:  0.10476994514465332\n",
            "needed time in inference-2:  0.1036374568939209\n",
            "needed time in inference-3:  0.10394096374511719\n",
            "needed time in inference-4:  0.10422563552856445\n",
            "needed time in inference-5:  0.10376834869384766\n",
            "needed time in inference-6:  0.10371851921081543\n",
            "needed time in inference-7:  0.10458111763000488\n",
            "needed time in inference-8:  0.10416817665100098\n",
            "needed time in inference-9:  0.10410642623901367\n",
            "needed time in inference-10:  0.1038661003112793\n",
            "needed time in inference-11:  0.10444188117980957\n",
            "needed time in inference-12:  0.10442829132080078\n",
            "needed time in inference-13:  0.10410308837890625\n",
            "needed time in inference-14:  0.1039273738861084\n",
            "needed time in inference-15:  0.10399174690246582\n",
            "needed time in inference-16:  0.10386371612548828\n",
            "needed time in inference-17:  0.10407757759094238\n",
            "needed time in inference-18:  0.1034536361694336\n",
            "needed time in inference-19:  0.10373830795288086\n",
            "needed time in inference-20:  0.10433483123779297\n",
            "needed time in inference-21:  0.10357832908630371\n",
            "needed time in inference-22:  0.10463166236877441\n",
            "needed time in inference-23:  0.10454082489013672\n",
            "needed time in inference-24:  0.10333585739135742\n",
            "needed time in inference-25:  0.10441350936889648\n",
            "needed time in inference-26:  0.1044471263885498\n",
            "needed time in inference-27:  0.10476207733154297\n",
            "needed time in inference-28:  0.10465168952941895\n",
            "needed time in inference-29:  0.10355663299560547\n",
            "needed time in inference-30:  0.10398006439208984\n",
            "needed time in inference-31:  0.10432839393615723\n",
            "needed time in inference-32:  0.10401749610900879\n",
            "needed time in inference-33:  0.10430622100830078\n",
            "needed time in inference-34:  0.10445618629455566\n",
            "needed time in inference-35:  0.10443425178527832\n",
            "needed time in inference-36:  0.10438179969787598\n",
            "needed time in inference-37:  0.10392189025878906\n",
            "needed time in inference-38:  0.10442709922790527\n",
            "needed time in inference-39:  0.10365724563598633\n",
            "needed time in inference-40:  0.10446882247924805\n",
            "needed time in inference-41:  0.1043093204498291\n",
            "needed time in inference-42:  0.10402035713195801\n",
            "needed time in inference-43:  0.10386133193969727\n",
            "needed time in inference-44:  0.1036539077758789\n",
            "needed time in inference-45:  0.10359930992126465\n",
            "needed time in inference-46:  0.10437846183776855\n",
            "needed time in inference-47:  0.10483860969543457\n",
            "needed time in inference-48:  0.10848402976989746\n",
            "needed time in inference-49:  0.10430502891540527\n",
            "average inference time of fp32:  0.10427139282226562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFmQ1ccSxHAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7ce1e5d6-aa54-44f5-b7b6-b5f486586548"
      },
      "source": [
        "\n",
        "trt_graph = trt.create_inference_graph(\n",
        "    input_graph_def=frozen_graph,        # frozen model\n",
        "    outputs=['dense_1/Softmax:0'],\n",
        "    max_batch_size=70,           # specify your max batch size\n",
        "    max_workspace_size_bytes=2*(10**9),    # specify the max workspace\n",
        "    precision_mode=\"FP16\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
        "\n",
        "#write the TensorRT model to be used later for inference\n",
        "with gfile.FastGFile(\"chest_fp6_model.pb\", 'wb') as f:\n",
        "    f.write(trt_graph.SerializeToString())\n",
        "print(\"TensorRT model is successfully stored!\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
            "TensorRT model is successfully stored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CXYBP9SxbH5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "c6da448f-f5d1-4a9b-9f30-203ed3416631"
      },
      "source": [
        "# variable\n",
        "TENSORRT_MODEL_PATH = 'chest_fp6_model.pb'\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "        # read TensorRT model\n",
        "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
        "\n",
        "        # obtain the corresponding input-output tensor\n",
        "        tf.import_graph_def(trt_graph, name='')\n",
        "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
        "        output = sess.graph.get_tensor_by_name('dense_1/Softmax:0')\n",
        "\n",
        "        # in this case, it demonstrates to perform inference for 50 times\n",
        "        total_time = 0; n_time_inference = 50\n",
        "        out_pred16 = sess.run(output, feed_dict={input: input_img})\n",
        "        for i in range(n_time_inference):\n",
        "            t1 = time.time()\n",
        "            out_pred_16 = sess.run(output, feed_dict={input: input_img})\n",
        "            t2 = time.time()\n",
        "            delta_time = t2 - t1\n",
        "            total_time += delta_time\n",
        "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
        "        avg_time_tensorRT_16 = total_time / n_time_inference\n",
        "        print(\"average inference time of fp16: \", avg_time_tensorRT_16)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needed time in inference-0:  0.10364818572998047\n",
            "needed time in inference-1:  0.10480713844299316\n",
            "needed time in inference-2:  0.10406947135925293\n",
            "needed time in inference-3:  0.10318708419799805\n",
            "needed time in inference-4:  0.10435891151428223\n",
            "needed time in inference-5:  0.1040041446685791\n",
            "needed time in inference-6:  0.10407519340515137\n",
            "needed time in inference-7:  0.10393071174621582\n",
            "needed time in inference-8:  0.1047062873840332\n",
            "needed time in inference-9:  0.10375857353210449\n",
            "needed time in inference-10:  0.10382652282714844\n",
            "needed time in inference-11:  0.103363037109375\n",
            "needed time in inference-12:  0.1042628288269043\n",
            "needed time in inference-13:  0.10347104072570801\n",
            "needed time in inference-14:  0.10330963134765625\n",
            "needed time in inference-15:  0.10398602485656738\n",
            "needed time in inference-16:  0.10500597953796387\n",
            "needed time in inference-17:  0.10410189628601074\n",
            "needed time in inference-18:  0.10398745536804199\n",
            "needed time in inference-19:  0.10553669929504395\n",
            "needed time in inference-20:  0.10509848594665527\n",
            "needed time in inference-21:  0.10368466377258301\n",
            "needed time in inference-22:  0.10431933403015137\n",
            "needed time in inference-23:  0.10322904586791992\n",
            "needed time in inference-24:  0.10402321815490723\n",
            "needed time in inference-25:  0.10428810119628906\n",
            "needed time in inference-26:  0.10402822494506836\n",
            "needed time in inference-27:  0.10407137870788574\n",
            "needed time in inference-28:  0.10336160659790039\n",
            "needed time in inference-29:  0.10430908203125\n",
            "needed time in inference-30:  0.10439634323120117\n",
            "needed time in inference-31:  0.10493111610412598\n",
            "needed time in inference-32:  0.10472321510314941\n",
            "needed time in inference-33:  0.10402560234069824\n",
            "needed time in inference-34:  0.10376834869384766\n",
            "needed time in inference-35:  0.10462188720703125\n",
            "needed time in inference-36:  0.10361242294311523\n",
            "needed time in inference-37:  0.10423517227172852\n",
            "needed time in inference-38:  0.10375118255615234\n",
            "needed time in inference-39:  0.10335373878479004\n",
            "needed time in inference-40:  0.10377120971679688\n",
            "needed time in inference-41:  0.10370826721191406\n",
            "needed time in inference-42:  0.10380935668945312\n",
            "needed time in inference-43:  0.10459494590759277\n",
            "needed time in inference-44:  0.10493040084838867\n",
            "needed time in inference-45:  0.10779595375061035\n",
            "needed time in inference-46:  0.10364532470703125\n",
            "needed time in inference-47:  0.1036674976348877\n",
            "needed time in inference-48:  0.10393571853637695\n",
            "needed time in inference-49:  0.10514569282531738\n",
            "average inference time of fp16:  0.10416466712951661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPQlllrsxk7V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a48a3de0-952a-4867-d72f-cfc39b02c38e"
      },
      "source": [
        "#calibraton\n",
        "converter = trt.TrtGraphConverter(\n",
        "    input_graph_def=graph_def,\n",
        "    nodes_blacklist=['dense_1/Softmax:0'],\n",
        "    precision_mode='INT8',\n",
        "    use_calibration=True)\n",
        "frozen_graph = converter.convert()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
            "WARNING:tensorflow:INT8 precision mode with calibration is supported with dynamic TRT ops only. Disregarding is_dynamic_op parameter.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUZXVVLWxrJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frozen_graph = converter.calibrate(\n",
        "    fetch_names=['dense_1/Softmax:0'],\n",
        "    num_runs=1,\n",
        "    feed_dict_fn=lambda: {'input_1:0': input_img})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdfbySp6xusi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.platform import gfile\n",
        "with gfile.FastGFile(\"chest_TensorRT_int8.pb\", 'wb') as f:\n",
        "    f.write(frozen_graph.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygOsGYoHxzVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "outputId": "c87f094a-15aa-4f0a-d3aa-fd2e9c689fe5"
      },
      "source": [
        "# variable\n",
        "TENSORRT_MODEL_PATH = 'chest_TensorRT_int8.pb'\n",
        "\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "        # read TensorRT model\n",
        "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
        "\n",
        "        # obtain the corresponding input-output tensor\n",
        "        tf.import_graph_def(trt_graph, name='')\n",
        "        input = sess.graph.get_tensor_by_name('input_1:0')\n",
        "        output = sess.graph.get_tensor_by_name('dense_1/Softmax:0')\n",
        "\n",
        "        # in this case, it demonstrates to perform inference for 50 times\n",
        "        total_time = 0; n_time_inference = 50\n",
        "        out_pred_8 = sess.run(output, feed_dict={input: input_img})\n",
        "        for i in range(n_time_inference):\n",
        "            t1 = time.time()\n",
        "            out_pred_8 = sess.run(output, feed_dict={input: input_img})\n",
        "            t2 = time.time()\n",
        "            delta_time = t2 - t1\n",
        "            total_time += delta_time\n",
        "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
        "        avg_time_tensorRT_int8 = total_time / n_time_inference\n",
        "        print(\"average inference time of int8: \", avg_time_tensorRT_int8)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needed time in inference-0:  0.1064608097076416\n",
            "needed time in inference-1:  0.10442042350769043\n",
            "needed time in inference-2:  0.10556793212890625\n",
            "needed time in inference-3:  0.10585331916809082\n",
            "needed time in inference-4:  0.10423660278320312\n",
            "needed time in inference-5:  0.10444426536560059\n",
            "needed time in inference-6:  0.1036379337310791\n",
            "needed time in inference-7:  0.10377073287963867\n",
            "needed time in inference-8:  0.10321283340454102\n",
            "needed time in inference-9:  0.10351419448852539\n",
            "needed time in inference-10:  0.1033315658569336\n",
            "needed time in inference-11:  0.10394024848937988\n",
            "needed time in inference-12:  0.10404777526855469\n",
            "needed time in inference-13:  0.10418415069580078\n",
            "needed time in inference-14:  0.10410928726196289\n",
            "needed time in inference-15:  0.10685420036315918\n",
            "needed time in inference-16:  0.10350227355957031\n",
            "needed time in inference-17:  0.10400581359863281\n",
            "needed time in inference-18:  0.10347819328308105\n",
            "needed time in inference-19:  0.10390806198120117\n",
            "needed time in inference-20:  0.10416626930236816\n",
            "needed time in inference-21:  0.1042027473449707\n",
            "needed time in inference-22:  0.10388922691345215\n",
            "needed time in inference-23:  0.10361456871032715\n",
            "needed time in inference-24:  0.10396575927734375\n",
            "needed time in inference-25:  0.10462188720703125\n",
            "needed time in inference-26:  0.10422563552856445\n",
            "needed time in inference-27:  0.1037607192993164\n",
            "needed time in inference-28:  0.10360193252563477\n",
            "needed time in inference-29:  0.10359835624694824\n",
            "needed time in inference-30:  0.1037132740020752\n",
            "needed time in inference-31:  0.10338091850280762\n",
            "needed time in inference-32:  0.1036064624786377\n",
            "needed time in inference-33:  0.1033487319946289\n",
            "needed time in inference-34:  0.1038820743560791\n",
            "needed time in inference-35:  0.10419893264770508\n",
            "needed time in inference-36:  0.10363340377807617\n",
            "needed time in inference-37:  0.10388398170471191\n",
            "needed time in inference-38:  0.10430026054382324\n",
            "needed time in inference-39:  0.10424208641052246\n",
            "needed time in inference-40:  0.10458493232727051\n",
            "needed time in inference-41:  0.1036839485168457\n",
            "needed time in inference-42:  0.10470795631408691\n",
            "needed time in inference-43:  0.10384225845336914\n",
            "needed time in inference-44:  0.10370445251464844\n",
            "needed time in inference-45:  0.1035621166229248\n",
            "needed time in inference-46:  0.10379600524902344\n",
            "needed time in inference-47:  0.10412716865539551\n",
            "needed time in inference-48:  0.10366535186767578\n",
            "needed time in inference-49:  0.10559654235839844\n",
            "average inference time of int8:  0.10411177158355713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBvPaWLcx2rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}